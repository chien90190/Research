{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GT6SJNt7Tkf",
        "colab_type": "text"
      },
      "source": [
        "#**Classification**\n",
        "\n",
        "https://github.com/qizhex/Controllable-Invariance\n",
        "\n",
        "For our classification experiments, the input is either a picture or a feature vector.\n",
        "All of the three players in the minimax game are constructed by feedforward neural networks. We feed s to the encoder as an embedding vector. We use the Extended Yale B dataset [Georghiades et al., 2001] for our image\n",
        "classification task. \n",
        "\n",
        "**Dataset source:** http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html\n",
        "\n",
        "It comprises face images of **38 people under 5 different lighting conditions**: upper right, lower right, lower left, upper left, or the front.\n",
        "\n",
        "* The **variable s** to be purged is the **lighting condition**. \n",
        "* The **label y** is the identity of the **person**. \n",
        "\n",
        "\n",
        "* We follow Li et al. [2014], Louizos et al. [2016]’s train/test split and **no validation** is used. \n",
        "* **38 × 5 = 190 samples** are used for training and all other **1, 096 data** points are used for testing.\n",
        "* We use a one-layer neural network for the encoder and a one-layer neural network for prediction. \n",
        "* γ is set to 2. \n",
        "* The discriminator is a two-layer neural network with batch normalization. \n",
        "* The batch size is set to 16 and the hidden size is set to 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m284uCNT7R85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ffac90b6-c5d4-430b-ddc1-661b150ee346"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import cuda\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as torchdata\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import copy\n",
        "from collections import deque\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%time\n",
        "\n",
        "import time\n",
        "import sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Found GPU at: {}'.format(device))\n",
        "\n",
        "torch.set_default_tensor_type('torch.cuda.DoubleTensor')\n",
        "# mount gdrive and confrim that dataset exists in this directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "path = 'gdrive/My Drive/Research/Projects/Classification/'"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.25 µs\n",
            "Found GPU at: cuda:0\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWHfLbi19mIE",
        "colab_type": "text"
      },
      "source": [
        "## Neural_Network\n",
        "Encoder, Predictor, and Discriminator\n",
        "\n",
        "* We use a one-layer neural network for the encoder and a one-layer neural network for prediction.\n",
        "\n",
        "* The discriminator is a two-layer neural network with batch normalization.\n",
        "\n",
        "* The batch size is set to 16 and the hidden size is set to 100\n",
        "\n",
        "* **one-layer neural network:** https://colab.research.google.com/github/KrishnaswamyLab/SingleCellWorkshop/blob/master/exercises/Deep_Learning/notebooks/00_Advanced_Cell_type_classification_with_neural_networks.ipynb#scrollTo=5KNIbIWld4W9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43NewA0rLW8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class layer(nn.Module):\n",
        "    def __init__(self, input_size, output_size, activation=None):\n",
        "        super(layer, self).__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.randn(input_size, output_size).double(), requires_grad=True)\n",
        "        self.bias = nn.Parameter(torch.randn(output_size).double(), requires_grad=True)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.matmul(x, self.weight) + self.bias\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2EXZA_aNhar",
        "colab_type": "text"
      },
      "source": [
        "### Encoder()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pqEjdlbNrLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim = 10, output_dim = 10, activation_function = None):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        # layers\n",
        "        self.L1 = layer(input_dim, output_dim, activation=activation_function)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # o = self.flatten(X)\n",
        "        o = self.L1(X)\n",
        "        # o = self.L2(o)\n",
        "        return o\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + torch.exp(-s))\n",
        "    \n",
        "    def flatten(self, x):\n",
        "        if len(x.size()) == 3:\n",
        "          B, _, _ = x.size()\n",
        "        else:\n",
        "          B, _, _, _ = x.size()\n",
        "\n",
        "        return x.contiguous().view(B,-1)"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3tzoQsWNeEA",
        "colab_type": "text"
      },
      "source": [
        "### Decoder()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDdqwLH-NsDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim = 1, output_dim = 3, hidden_size = 1, activation_function = None):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        # layers\n",
        "        self.L1 = layer(input_dim, hidden_size, activation = activation_function)\n",
        "        self.L2 = layer(hidden_size, output_dim)\n",
        "        # self.L2 = nn.Linear(num_filters, output_dum)\n",
        "\n",
        "    def forward(self, X):\n",
        "        o = self.flatten(X)\n",
        "        o = self.L1(o)\n",
        "        self.z = self.L2(o)\n",
        "        return self.z\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + torch.exp(-s))\n",
        "\n",
        "    def flatten(self, x):\n",
        "        # print (x.size())\n",
        "        if len(x.size()) == 3:\n",
        "          B, _, _ = x.size()\n",
        "        elif len(x.size()) == 4:\n",
        "          B, _, _, _ = x.size()\n",
        "        else:\n",
        "          B, _ = x.size()\n",
        "\n",
        "        return x.contiguous().view(B,-1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCrm8eHDNjfP",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWBueMF3VU7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim = 1, output_dim = 3, hidden_size = 1):\n",
        "        super(Discriminator, self).__init__()\n",
        "    \n",
        "        assert (len(hidden_size) == 3)\n",
        "        # layers\n",
        "        self.L1 = layer(input_dim, hidden_size[0], activation=nn.Sigmoid())\n",
        "        self.L2 = layer(hidden_size[0], hidden_size[1], activation=nn.Sigmoid())\n",
        "        self.L3 = layer(hidden_size[1], output_dim)#, activation=nn.Sigmoid())\n",
        "        self.B1 = nn.BatchNorm1d(hidden_size[1])\n",
        "        # self.L4 = layer(hidden_size[2], output_dim)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        o = self.L1(X)\n",
        "        o = self.L2(o)\n",
        "        o = self.B1(o)\n",
        "        o = self.L3(o)\n",
        "        # o = self.L4(o)\n",
        "        return o\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + torch.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        # derivative of sigmoid\n",
        "        return s * (1 - s)\n",
        "\n",
        "    def flatten(self, x):\n",
        "        # print (x.size())\n",
        "        B, N, _, _ = x.size()\n",
        "        return x.contiguous().view(B,-1)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x1uQtBTkY40",
        "colab_type": "text"
      },
      "source": [
        "### NMT_Mode(), Combine_model()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmt7uKWwjipn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Combine_model(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Combine_model, self).__init__()\n",
        "\n",
        "        #  layer\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, X):\n",
        "        o = self.encoder(X)\n",
        "        o = self.decoder(o)\n",
        "        return o"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZCG-45jOthP",
        "colab_type": "text"
      },
      "source": [
        "**NMTModel()**\n",
        "\n",
        "Train all three components together with a gradient reversal layer\n",
        "**[[Ganin and Lempitsky, 2015]](http://proceedings.mlr.press/v37/ganin15.pdf)**\n",
        "\n",
        "* The proposed architecture includes a **deep feature extractor (green)** and a **deep label predictor (blue)**, which together form a standard feed-forward architecture. \n",
        "\n",
        "* Unsupervised domain adaptation is achieved by adding **a domain classifier (red)** connected to the feature extractor via a **gradient reversal layer** that **multiplies the gradient by a certain negative constant** during the backpropagationbased training. \n",
        "\n",
        "* Otherwise, the training proceeds in a standard way and minimizes the label prediction loss (for source examples) and\n",
        "the domain classification loss (for all samples). \n",
        "\n",
        "* Gradient reversal ensures that the feature distributions over the two domains are made similar (as indistinguishable as possible for the domain classifier), thus resulting in the domain-invariant features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZakrhBOVDAlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NMTModel(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, discriminator, generator = 0):\n",
        "        super(NMTModel, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.dec_grad_norm = 0        \n",
        "\n",
        "    def get_disc_parameters(self):\n",
        "        for comp in [self.discriminator]:\n",
        "            if comp is None:\n",
        "                continue\n",
        "            for p in comp.parameters():\n",
        "                yield p\n",
        "\n",
        "    def get_encoder_parameters(self):\n",
        "        for comp in [self.encoder]:\n",
        "            for p in comp.parameters():\n",
        "                yield p\n",
        "\n",
        "    def make_init_decoder_output(self, context):\n",
        "        batch_size = context.size(1)\n",
        "        h_size = [batch_size]\n",
        "        return Variable(context.data.new(*h_size).zero_(), requires_grad=False)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        context = self.encoder(input) #input to encoder \n",
        "        # init_output = self.make_init_decoder_output(context) #input to decoder?\n",
        "\n",
        "        #  how does it works     \n",
        "        dec_norm = []\n",
        "        # dec_context_variable.register_hook(dec_wrapper(dec_norm))\n",
        "\n",
        "        #  output of discriminator\n",
        "        disc_out = self.discriminator(context)\n",
        "                \n",
        "        #  output of decoder\n",
        "        out = self.decoder(context) #dec_context_variable)\n",
        "\n",
        "\n",
        "        return out, disc_out, dec_norm\n",
        "        \n",
        "\n",
        "def dec_wrapper(norm):\n",
        "    def hook_func(grad):\n",
        "        norm.append(math.pow(grad.norm().data[0], 2))\n",
        "        pass\n",
        "    return hook_func"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgN66Vy5pCfZ",
        "colab_type": "text"
      },
      "source": [
        "NMTCriterion()\n",
        "\n",
        "https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb\n",
        "\n",
        "**nn.NLLLoss** calculates the probability of the correct label.  --> probability(correct_label)\n",
        "\n",
        "e.g. [0.1, 0.8, 0.1]\n",
        "\n",
        "**nn.CrossEntropLoss** It is: log(probability(correct_label)), summed over the minibatch. So output is between 0 and 2 (since you have 3 labels).\n",
        "\n",
        "e.g. [1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePWtAtoKpCmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NMTCriterion(the_type = 'cel', target_size = 2):\n",
        "    \n",
        "    # weight = torch.Tensor(weight)\n",
        "    if the_type == 'cel':\n",
        "      crit = nn.CrossEntropyLoss().double()\n",
        "    # elif the_type == 'cross_entropy_loss_logit':\n",
        "    #   crit = the_typ\n",
        "    else:\n",
        "      crit = nn.NLLLoss().double()\n",
        "      print ('nllloss')\n",
        "\n",
        "    return crit"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4MrahBMPhzk",
        "colab_type": "text"
      },
      "source": [
        "https://discuss.pytorch.org/t/how-to-print-the-computed-gradient-values-for-a-network/34179\n",
        "\n",
        "weight.grad.data[0] -> gradient  dL/dweight\n",
        "\n",
        "a.resgister_hook(lambda grad: print(grad)) --> get the  dL/da"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJHRf-ACCcT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder = Encoder(1,100,50,3)\n",
        "# predictor = Decoder(50,30,38,3, 38*188*164)\n",
        "# discrminator = Discriminator(50,5,30, 20,3, 186*162*20)\n",
        "\n",
        "# models = [NMTModel(encoder, predictor, discrminator).float()]\n",
        "# summary(models[0], (1,192,168),16)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzcW8qEOv9KV",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4D6sNASv9XD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Optim(object):\n",
        "\n",
        "    def _makeOptimizer(self):\n",
        "        if self.method == 'sgd':\n",
        "            self.optimizer = optim.SGD(self.params, lr=self.lr)\n",
        "        elif self.method == 'adagrad':\n",
        "            self.optimizer = optim.Adagrad(self.params, lr=self.lr)\n",
        "        elif self.method == 'adadelta':\n",
        "            self.optimizer = optim.Adadelta(self.params, lr=self.lr)\n",
        "        elif self.method == 'adam':\n",
        "            self.optimizer = optim.Adam(self.params, lr=self.lr, betas=(self.adam_momentum, 0.999))\n",
        "        else:\n",
        "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
        "\n",
        "    def __init__(self, params, disc_params, enc_params, method, max_grad_norm = 5, lr = 0.001, lr_decay=1, start_decay_at=None, adam_momentum=0.9):\n",
        "        self.params = list(params)  # careful: params may be a generator\n",
        "        self.disc_params = list(disc_params)\n",
        "        self.encoder_params = list(enc_params)\n",
        "        self.last_ppl = None\n",
        "        self.lr = lr\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.method = method\n",
        "        self.lr_decay = lr_decay\n",
        "        self.start_decay_at = start_decay_at\n",
        "        self.start_decay = False\n",
        "        self.adam_momentum = adam_momentum\n",
        "        self._makeOptimizer()\n",
        "\n",
        "    def step_all(self):\n",
        "        #  Compute gradients norm.\n",
        "        #     Gradients are not “initialize”, they are equal to None until you backpropagate something.\n",
        "        #     You can access to gradients by doing w.grad\n",
        "        #     However nothing will be returned as there are no gradients… (unless you backprop an scalar which involves w)\n",
        "        \n",
        "        enc_grad_norm = 0\n",
        "        # for param in self.disc_params: # encoder\n",
        "        #     enc_grad_norm += math.pow(param.grad.data.norm(), 2)   \n",
        "        # enc_grad_norm = math.sqrt(enc_grad_norm)\n",
        "\n",
        "        grad_norm = 0\n",
        "        # for param in self.params: # model parameter\n",
        "        #     grad_norm += math.pow(param.grad.data.norm(), 2)\n",
        "        # # grad_norm = math.sqrt(grad_norm)\n",
        "\n",
        "        # # shrinkage = self.max_grad_norm / grad_norm\n",
        "        # for param in self.params:\n",
        "        #     if shrinkage < 1:\n",
        "        #         param.grad.data.mul_(shrinkage)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return enc_grad_norm, grad_norm\n",
        "\n",
        "    def step_disc_enc(self, disc_lambda):\n",
        "        # Compute gradients norm.\n",
        "        enc_grad_norm = 0\n",
        "        grad_norm = 0\n",
        "\n",
        "        for param in self.encoder_params:\n",
        "            n = math.pow(param.grad.data.norm(), 2)\n",
        "            if math.fabs(disc_lambda) > 1e-5:\n",
        "                n *= math.pow(disc_lambda, 2)\n",
        "            param.grad.data.mul_(-disc_lambda)\n",
        "            enc_grad_norm += n\n",
        "            grad_norm += n\n",
        "        enc_grad_norm = math.sqrt(enc_grad_norm)\n",
        "        for param in self.disc_params:\n",
        "            grad_norm += math.pow(param.grad.data.norm(), 2)\n",
        "        grad_norm = math.sqrt(grad_norm)\n",
        "        if grad_norm > 0:\n",
        "            shrinkage = self.max_grad_norm / grad_norm\n",
        "        else:\n",
        "            shrinkage = 1.\n",
        "\n",
        "        for param in self.encoder_params + self.disc_params:\n",
        "            if shrinkage < 1:\n",
        "                param.grad.data.mul_(shrinkage)\n",
        "\n",
        "        self.optimizer.step()\n",
        "        return enc_grad_norm, grad_norm\n",
        "\n",
        "    # decay learning rate if val perf does not improve or we hit the start_decay_at limit\n",
        "    # def updateLearningRate(self, ppl, epoch):\n",
        "\n",
        "    #     if self.start_decay_at is not None and epoch >= self.start_decay_at:\n",
        "    #         self.start_decay = True\n",
        "    #     if self.last_ppl is not None and ppl > self.last_ppl:\n",
        "    #         self.start_decay = True\n",
        "\n",
        "    #     if self.start_decay:\n",
        "    #         self.lr = self.lr * self.lr_decay\n",
        "    #         print(\"Decaying learning rate to %g\" % self.lr)\n",
        "\n",
        "    #     self.last_ppl = ppl\n",
        "    #     self._makeOptimizer()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANllB33kGCy3",
        "colab_type": "text"
      },
      "source": [
        "**Training**\n",
        "\n",
        "train all three components together with a gradient reversal layer\n",
        "[Ganin and Lempitsky, 2015]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q4uZ7YgL4zM",
        "colab_type": "text"
      },
      "source": [
        "## Combination_Loss()\n",
        "\n",
        "https://discuss.pytorch.org/t/basics-of-loss-backward/14069\n",
        "\n",
        "This won’t actually compute any gradients. It detaches loss from the rest of the computation graph; you’re creating a new node that is loss2 = Variable(loss.data) that is not connected to the rest of the computation graph.\n",
        "\n",
        "https://stackoverflow.com/questions/46774641/what-does-the-parameter-retain-graph-mean-in-the-variables-backward-method\n",
        "\n",
        "As long as you use retain_graph=True in your backward method, you can do backward any time you want\n",
        "\n",
        "https://stackoverflow.com/questions/54754153/autograd-grad-for-tensor-in-pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ9FvANSL48a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Combination_Loss(outputs, public_label, disc_out, sensitive_label, criterion, discriminator_criterion, eval=False, print_prob=False, dicts=None):\n",
        "\n",
        "    #  calculate encoder and decoder loss\n",
        "    outputs = Variable(outputs.data, requires_grad=(not eval)).contiguous() # continguous before linear layer?    \n",
        "    dec_loss = criterion(outputs, public_label.long()) # calculate the total loss\n",
        "    \n",
        "    if not eval:\n",
        "      dec_loss.backward()\n",
        "\n",
        "    #  get the decoder gradient\n",
        "    dec_grad_output = None if outputs.grad is None else outputs.grad.data\n",
        "      \n",
        "    #  discrimiinator loss\n",
        "    disc_out = Variable(disc_out.data, requires_grad=(not eval)).contiguous()\n",
        "    disc_loss = gamma* discriminator_criterion(disc_out, sensitive_label.long()) # calculate the total loss\n",
        "    \n",
        "    if not eval:\n",
        "      disc_loss.backward() # division\n",
        "\n",
        "    #  get the discrminator gradient\n",
        "    grad_disc_out = None if disc_out.grad is None else disc_out.grad.data\n",
        "\n",
        "    return dec_loss, disc_loss, dec_grad_output, grad_disc_out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj-73URXrKDF",
        "colab_type": "text"
      },
      "source": [
        "## prediction(), evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkoG54HTrKLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(output, label):\n",
        "  scores = F.softmax(output, dim = 1)\n",
        "  probability, indecies = torch.max(scores,dim=1)\n",
        "  train_total = label.size(0)\n",
        "  train_correct = np.sum(np.array(label.cpu())==np.array(indecies.cpu()))\n",
        "\n",
        "  return train_correct, train_total, indecies"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApB2eRXOb4zq",
        "colab_type": "text"
      },
      "source": [
        "Evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKSS2Vz8b49H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation(models, criterion, discriminator_criterion, valid_loader, model_index = 0):\n",
        "\n",
        "    total_loss = 0\n",
        "    total_disc_acc = 0\n",
        "    total_sent = 0\n",
        "    total_correct, total_number = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    #  set to evaluation mode\n",
        "    models[model_index].eval()\n",
        "\n",
        "    for i, (data, labels) in enumerate(valid_loader):\n",
        "        \n",
        "        # labels\n",
        "        data = Variable(data).double().to(device)\n",
        "        public_label = Variable(labels[:,0]).to(device)\n",
        "        sensitive_label = Variable(labels[:,1]).to(device)\n",
        "\n",
        "        # decoder and discriminator outputs\n",
        "        outputs, disc_out, dec_norm = models[0](data)\n",
        "\n",
        "        # loss function\n",
        "        dec_loss, _, _, _ = Combination_Loss(outputs, public_label, disc_out, sensitive_label,\n",
        "                                             criterion, discriminator_criterion, eval = True, print_prob=False) \n",
        "       \n",
        "        # prediction \n",
        "        correct, _, _ = prediction(outputs, public_label.long())\n",
        "        disc_correct, _, _ = prediction(disc_out, sensitive_label.long())\n",
        "        total_correct += correct\n",
        "\n",
        "        # accumulate results\n",
        "        total_loss += dec_loss\n",
        "        total_disc_acc += disc_correct\n",
        "        total_sent += data.size(0)\n",
        "        count += 1\n",
        "    \n",
        "    #  retrun to train mode\n",
        "    models[model_index].train()\n",
        "\n",
        "    return total_loss/count, total_correct/total_sent, total_disc_acc/ total_sent"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggfTNX3hpGVq",
        "colab_type": "text"
      },
      "source": [
        "## Train_Model(), Test_Model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGPYdpTNuanZ",
        "colab_type": "text"
      },
      "source": [
        "they split the loss function to a subgraph \n",
        "\n",
        "--> calculate the graidents of the subgraph and the backprop this obtained gradient back to original model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E3ICtDTnXNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Test_Model(models, test_loader, printing = 10, verbose = False):\n",
        "\n",
        "  total_correct, total_disc_acc, total_sent = 0, 0, 0\n",
        "\n",
        "  for i, (data, labels) in enumerate(test_loader):\n",
        "\n",
        "          # labels\n",
        "          data = data.double().to(device)\n",
        "          public_label = labels[:,0].to(device)\n",
        "          sensitive_label = labels[:,1].to(device)\n",
        "\n",
        "          # decoder and discriminator outputs\n",
        "          outputs, disc_out, dec_norm = models[0](data)\n",
        "\n",
        "          # prediction \n",
        "          correct, _, indecies = prediction(outputs, public_label)\n",
        "          disc_correct, _, disc_indecies = prediction(disc_out, sensitive_label)\n",
        "          total_correct += correct\n",
        "\n",
        "          # accumulate results\n",
        "          total_disc_acc += disc_correct\n",
        "          total_sent += public_label.size(0)\n",
        "\n",
        "          if verbose and i == 0:\n",
        "            print('\\npublic   ',public_label[:printing])\n",
        "            print('pub pred.',indecies[:printing])\n",
        "            print('sensitive',sensitive_label[:printing])\n",
        "            print('sen pred.',disc_indecies[:printing])\n",
        "\n",
        "  dec_acc = total_correct/total_sent\n",
        "  dis_acc = total_disc_acc/ total_sent\n",
        "  print('Test Accuracies:: (pub)dec_acc: %f, (sen)disc_acc: %f' % (total_correct/total_sent, total_disc_acc/ total_sent))\n",
        "  print()\n",
        "\n",
        "  return dec_acc, dis_acc"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynGj1qX6toA9",
        "colab_type": "text"
      },
      "source": [
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNRBPAattmuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Train_Model(models, train_loader, valid_loader, loss_type, optims, epochs, batch_size, print_every = 10, verbose = False):\n",
        "\n",
        "    #  set to training mode\n",
        "    for single_model in models:\n",
        "        single_model.double().train()\n",
        "\n",
        "    # define criterion/loss function\n",
        "    print ('* Predictor     Criterion:', loss_type)\n",
        "    criterion = NMTCriterion(loss_type)\n",
        "    print ('* Discriminator Criterion:', loss_type, '\\n')\n",
        "    discriminator_criterion = NMTCriterion(loss_type) #nn.NLLLoss(reduction='sum') #cuda\n",
        "\n",
        "    history_valid = []\n",
        "    count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        if verbose:\n",
        "          print ('\\nEpoch %i:' %(epoch+1))\n",
        "\n",
        "        #  ================================= Training =================================  #\n",
        "\n",
        "        total_loss, total_disc_acc = 0, 0\n",
        "        total_sent, total_dec_acc = 0, 0\n",
        "        train_total, train_correct = 0, 0\n",
        "        total = 0\n",
        "\n",
        "        for k, (data, labels) in enumerate(train_loader):\n",
        "            \n",
        "            model_index = 0\n",
        "            data = data.double().to(device)\n",
        "            public_label = Variable(labels[:,0]).double().to(device)     # index 0 = public_label\n",
        "            sensitive_label = Variable(labels[:,1]).double().to(device)  # index 1 = sensitive_label\n",
        "\n",
        "            #  create zero gradients\n",
        "            models[model_index].zero_grad()\n",
        "\n",
        "            #  forward propagation\n",
        "            outputs, disc_out, dec_norm = models[model_index](data)\n",
        "\n",
        "            #  calculate loss + gradients until the output layer\n",
        "            dec_loss, disc_loss, grad_dec_output, grad_disc_output = Combination_Loss(outputs, public_label, disc_out, sensitive_label,\n",
        "                                                                                criterion, discriminator_criterion, print_prob=False)            \n",
        "\n",
        "            #  backward propagation the rest of the layers with the above graidents\n",
        "            torch.autograd.backward([outputs, disc_out], [grad_dec_output, grad_disc_output])\n",
        "\n",
        "            _, grad_norm = optims[model_index].step_all()\n",
        "\n",
        "            #  calculate accuracy\n",
        "            dec_correct, dec_total, dec_indecies = prediction(outputs, public_label)\n",
        "            disc_correct, disc_total, disc_indecies = prediction(disc_out, sensitive_label)\n",
        "\n",
        "            #  save loss results\n",
        "            total_dec_acc += dec_correct\n",
        "            total_disc_acc += disc_correct\n",
        "            total_sent += data.size(0)\n",
        "            total_loss += dec_loss\n",
        "            total += 1\n",
        "            count += 1\n",
        "\n",
        "            if count % print_every == 0:\n",
        "                print('loss: %f, dec_acc: %f, disc_acc: %3f'% (dec_loss, dec_correct/dec_total, disc_correct/disc_total))\n",
        "                \n",
        "\n",
        "            # if verbose and (i % print_every == 0):\n",
        "            #     if verbose == 'dec':\n",
        "            #       print('decoder output:\\n', outputs[:printing])\n",
        "            #       print('decoder  grad.:\\n', grad_disc_output)\n",
        "            #       # print('decoder weight:\\n', models[0].decoder.z7)\n",
        "            #     else:\n",
        "            #       print('disc gradient:\\n', grad_disc_output[:printing])\n",
        "            #       print('disc  weights:\\n', models[0].discriminator.z7[:printing])\n",
        "            #       # print('input bacth data:\\n', data)\n",
        "            #       # print('discrimantor output:\\n', disc_out)\n",
        "            \n",
        "            # if verbose and (i % print_every == 0):\n",
        "            #     if verbose == 'dec':\n",
        "            #       print ('decoder pred:', dec_indecies[:printing])\n",
        "            #       print ('public label:', public_label[:printing])\n",
        "            #       print ('decoder accuracy:', dec_correct/dec_total)\n",
        "            #     else:\n",
        "            #       print ('discriman predic:', disc_indecies[:printing])\n",
        "            #       print ('sensitive labels:', sensitive_label[:printing])\n",
        "            #       print ('discriman accura:', disc_correct/disc_total, '\\n')\n",
        "\n",
        "\n",
        "        train_loss, dec_acc, disc_acc = float(total_loss)/float(total), total_dec_acc/total_sent, total_disc_acc / total_sent\n",
        "\n",
        "        #  ================================= Validation =================================  #\n",
        "\n",
        "        valid_loss, valid_dec_acc, valid_disc_acc = evaluation(models, criterion, discriminator_criterion, valid_loader)\n",
        "\n",
        "        if verbose: # and (epoch % print_every == 0):\n",
        "            print('\\nTrain loss: %f, (pub)dec_acc: %f, (sen)disc_acc: %f'%(train_loss, dec_acc, disc_acc))\n",
        "            print('Valid loss: %f, (pub)dec_acc: %f, (sen)disc_acc: %f' % (valid_loss, valid_dec_acc, valid_disc_acc))\n",
        "\n",
        "\n",
        "        # #  (3) maybe update the learning rate\n",
        "        # if opt.optim == 'sgd':\n",
        "        #     for single_optim in optims:\n",
        "        #         single_optim.updateLearningRate(valid_loss, epoch)\n",
        "\n",
        "        history_valid.append(valid_loss)\n",
        "\n",
        "    return history_valid\n",
        "      "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOb9zN_3tUPN",
        "colab_type": "text"
      },
      "source": [
        "## main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUK4PD70tVFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(train_loader, valid_loader, encoder, predictor, discrminator, loss_type = 0, learning_rate = 0.01, batch_size = 16, epochs = 10, print_every = 10, verbose = False):\n",
        "\n",
        "    #  model\n",
        "    print ('Loading Model...')\n",
        "    models = [NMTModel(encoder, predictor, discrminator).double()]\n",
        "\n",
        "    if iterate == 0:\n",
        "      summary(models[0].float(), (1, input_size))\n",
        "      models[0].double()\n",
        "          \n",
        "    #  optimizor\n",
        "    print ('Loading Optimizor...')\n",
        "\n",
        "    for model in models:\n",
        "        optims = [Optim(model.parameters(), model.get_disc_parameters(), model.get_encoder_parameters(), 'adam', lr = learning_rate)]\n",
        "\n",
        "        # https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073\n",
        "        # for p in model.get_disc_parameters():\n",
        "        #     p.data.uniform_(-1, 1)\n",
        "\n",
        "    nParams = sum([p.nelement() for model in models for p in model.parameters()])\n",
        "    print('* number of parameters: %d' % nParams)\n",
        "\n",
        "    #  train\n",
        "    print ('\\nTraining Model....')\n",
        "    history_valid = Train_Model(models, train_loader, valid_loader, loss_type, optims, epochs, batch_size, print_every, verbose)\n",
        "\n",
        "    return models, history_valid"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiMeCbnp5YQg",
        "colab_type": "text"
      },
      "source": [
        "# Original Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R8vRrpkEW65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def original_training(model, train_loader, valid_loader, label_index = 0, gamma = 1.0, weight = 0, learning_rate = 0.01, epochs = 10, batch_size = 10, print_every = 10, printing = True):\n",
        "\n",
        "    #  set the model to type DoubleTensor\n",
        "    model.double()\n",
        "    print ('\\nTraining...  Label:', label_index)\n",
        "    \n",
        "    #  set up biased loss for biased dataset \n",
        "    weight = torch.Tensor([1.0,1.0]).double() if weight == 0 else torch.Tensor(weight).double()\n",
        "\n",
        "    #  define loss function and optimizoe\n",
        "    loss_fn = nn.CrossEntropyLoss(weight)\n",
        "    optimiztor = optim.Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    count = 0\n",
        "    loss_list, valid_loss_hist, iteration_list, accuracy_list = [], [], [], []\n",
        "    valid_acc_hist, train_acc_hist= [], []\n",
        "\n",
        "    # START TRAINING ====================================================================\n",
        "    for epcoh in range(epochs):\n",
        "\n",
        "        if printing:\n",
        "          print('Epoch: {}/{}'.format(epcoh+1, epochs))\n",
        "        \n",
        "        train_total = 0\n",
        "        train_correct = 0\n",
        "\n",
        "        for i, (data, labels) in enumerate(train_loader):\n",
        "\n",
        "          #  read data to the device\n",
        "          data = data.double().to(device)\n",
        "          label = labels[:,label_index].double().to(device)     # index 0 = public_label\n",
        "\n",
        "          #  create zero gradients\n",
        "          model.zero_grad()\n",
        "\n",
        "          #  forward propagation\n",
        "          prediction = model(data)\n",
        "\n",
        "          #  calculate gradients and loss\n",
        "          loss = gamma * loss_fn(prediction, label.long())  # calculate loss        \n",
        "          loss.backward()                                   # backward propagation to get graidents\n",
        "\n",
        "          if count % print_every == 0:\n",
        "            print('loss: {}'.format(loss))\n",
        "\n",
        "          #  apply gradients\n",
        "          optimiztor.step()\n",
        "        \n",
        "          #  calculate training accuracy\n",
        "          scores = F.softmax(prediction, dim=1)\n",
        "          probability, indecies = torch.max(scores,dim=1)\n",
        "          train_total += label.size(0)\n",
        "          train_correct += np.sum(np.array(label.cpu())==np.array(indecies.cpu()))\n",
        "          # print('label  :',label)\n",
        "          # print('predict:', indecies)\n",
        "\n",
        "          count += 1\n",
        "\n",
        "          #  Calculate Validation ==================================================     \n",
        "          if count % 10 == 0 and valid_loader:\n",
        "              correct = 0\n",
        "              valid_total = 0\n",
        "\n",
        "              for data, labels in valid_loader:\n",
        "                  \n",
        "                  #  Load Data frmo validation dataset\n",
        "                  data = data.double().to(device)\n",
        "                  label = labels[:,label_index].double().to(device)\n",
        "\n",
        "                  #  Prediction\n",
        "                  y_pred = model(data)\n",
        "                  valid_loss = loss_fn(y_pred, label.long())\n",
        "\n",
        "                  #  Get predictions probabilities using softmax\n",
        "                  scores = F.softmax(y_pred, dim=1)\n",
        "                  probability, indecies = torch.max(scores,dim=1)\n",
        "\n",
        "                  #  Total number of labels\n",
        "                  valid_total += label.size(0)\n",
        "                  correct += np.sum(np.array(label.cpu())==np.array(indecies.cpu()))\n",
        "\n",
        "              valid_loss_hist.append(valid_loss.data)\n",
        "              valid_acc = correct / float(valid_total)\n",
        "\n",
        "              # store loss and iteration\n",
        "              loss_list.append(loss.data)\n",
        "              iteration_list.append(count)\n",
        "              valid_acc_hist.append(valid_acc)\n",
        "\n",
        "        #  End Calculate Validation ================================================     \n",
        "        \n",
        "        train_acc = train_correct / float(train_total)\n",
        "        train_acc_hist.append(train_acc)\n",
        "\n",
        "        if printing:\n",
        "          print('Training Accuract: {}'.format(train_acc))\n",
        "          print('Validation Accuracy: {}'.format(valid_acc)) if valid_loader else None\n"
      ],
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1XnR79MGjuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def original_testing(model, test_loader, label_index, printing = True, printing_length = 10):\n",
        "\n",
        "    test_total, test_correct = 0, 0\n",
        "\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "\n",
        "        data = Variable(data).to(device)\n",
        "        label = Variable(labels[:,label_index]).to(device)     # index 0 = public_label\n",
        "\n",
        "        #  forward propagation\n",
        "        prediction = model(data)\n",
        "\n",
        "        #  calculate the accuracy\n",
        "        scores = F.softmax(prediction, dim=1)\n",
        "        probability, indecies = torch.max(scores,dim=1)\n",
        "        test_total += label.size(0)\n",
        "        test_correct += np.sum(np.array(label.cpu())==np.array(indecies.cpu()))\n",
        "\n",
        "        if i == 0 and printing:\n",
        "          print('\\nReal label:', label[:printing_length])\n",
        "          print('Pred label:', indecies[:printing_length])\n",
        "\n",
        "    test_acc = test_correct / float(test_total)\n",
        "    print('Test Accuracy: {}'.format(test_acc), '(total:%s)\\n'%(test_total))\n",
        "\n",
        "    return test_acc"
      ],
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_biPoHRFFf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loading_data(cleandata, cleanlabel, test_percent = 0.13, valid_precent = 0.1, shuffling = False, printing = True, valid_data = True, test_data = True):\n",
        "\n",
        "    print('Loading Dataset...')\n",
        "    if test_data:\n",
        "      train_x, test_x, train_y, test_y = train_test_split(cleandata, cleanlabel, test_size= test_percent, random_state=42)\n",
        "    else:\n",
        "      train_x, train_y = cleandata, cleanlabel\n",
        "\n",
        "    if valid_data:\n",
        "      train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size= valid_precent, random_state=42)\n",
        "\n",
        "    if printing:\n",
        "      print('   train dataset:', np.shape(train_x), np.shape(train_y))\n",
        "      if valid_data:\n",
        "        print('   valid dataset:', np.shape(valid_x), np.shape(valid_y))\n",
        "      if test_data:\n",
        "        print('   test  dataset:', np.shape(test_x), np.shape(test_y))\n",
        "\n",
        "    # training dataset\n",
        "    train_x, train_y = torch.from_numpy(train_x).double(), torch.from_numpy(train_y)\n",
        "    train_ds = torchdata.TensorDataset(train_x, train_y)\n",
        "    train_loader = torchdata.DataLoader(train_ds, batch_size=batch_size, shuffle=shuffling)\n",
        "\n",
        "    #  testing dataset\n",
        "    if test_data:\n",
        "      test_x, test_y = torch.from_numpy(test_x).double(), torch.from_numpy(test_y)\n",
        "      test_ds = torchdata.TensorDataset(test_x, test_y)\n",
        "      test_loader = torchdata.DataLoader(test_ds, batch_size=batch_size, shuffle=shuffling)\n",
        "    else: \n",
        "      test_loader = []\n",
        "\n",
        "    # validation dataset\n",
        "    if valid_data:\n",
        "      valid_x, valid_y = torch.from_numpy(valid_x).double(), torch.from_numpy(valid_y)\n",
        "      valid_ds = torchdata.TensorDataset(valid_x, valid_y)\n",
        "      valid_loader = torchdata.DataLoader(valid_ds, batch_size=batch_size, shuffle=shuffling)\n",
        "    else:\n",
        "      valid_loader = []\n",
        "\n",
        "    return train_loader, valid_loader, test_loader"
      ],
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpeYICAvGnwc",
        "colab_type": "text"
      },
      "source": [
        "# **German Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k5RsGVG1liU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#unit deivation      \n",
        "def StdScaler(data):\n",
        "    for i in range(len(data)):\n",
        "        data[i] = data[i]/np.std(data[i])\n",
        "    return data\n",
        "\n",
        "#zero mean and unit variance\n",
        "def zero_mean_unit_var(input_data):\n",
        "  means = np.mean(input_data, axis = 1).reshape(-1,1)\n",
        "  input_data = input_data - means\n",
        "  input_data = np.transpose(StdScaler(input_data))\n",
        "  return input_data\n",
        "\n",
        "# change the feature of 2 unique values into 0 and 1\n",
        "def target_2label_processing(dataset, index):\n",
        "    for indecies, items in enumerate(dataset[index].unique()):\n",
        "        print ('%s = %s'%(indecies, items))\n",
        "        dataset.loc[(dataset[index] == items, index)] = indecies\n",
        "\n",
        "    sensitive_column = []\n",
        "    for value in dataset[index]:\n",
        "        sensitive_column.append(value)\n",
        "    \n",
        "    sensitive_column = np.asarray(sensitive_column).reshape(-1,1)\n",
        "\n",
        "    return sensitive_column\n",
        "\n",
        "#change the categorical features in place\n",
        "def categorial_feature_change(datset, categorial_features):\n",
        "    for index in categorial_features:    \n",
        "        unique_values = datset[index].unique()\n",
        "        for i in range(len(unique_values)):\n",
        "            datset.loc[(datset[index] == unique_values[i], index)] = i\n",
        "\n",
        "# extract datas from input features\n",
        "def collect_columndata(raw_data, new_data, features):\n",
        "  for feature in features:\n",
        "    content = list(raw_data[feature])\n",
        "    new_data.append(content)\n",
        "  return new_data\n",
        "\n",
        "# one hot encoding\n",
        "def ohe_encoding(dataset, ohe_features):\n",
        "  the_onehot = dataset.iloc[:, ohe_features].values\n",
        "  the_onehot_labels = []\n",
        "\n",
        "  for i in range(len(ohe_features)):\n",
        "      values = np.unique(the_onehot[:,i])\n",
        "      for value in values:\n",
        "          the_onehot_labels.append( str(ohe_features[i]) +'_'+str(value))\n",
        "      \n",
        "  onehotencoder = OneHotEncoder(categories='auto')\n",
        "  the_onehot = onehotencoder.fit_transform(the_onehot).toarray()\n",
        "\n",
        "  return the_onehot, the_onehot_labels"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQbrv0zW7dAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5b8d4191-8fd9-4f04-f8a2-d61aec75a1d2"
      },
      "source": [
        "categorical_features = [0,2,3,5,6,9,11,13,14,16,18,19] # categorical\n",
        "numerical_features = [1,4,7,10,12,15,17]   # numerical\n",
        "label_features = [0,2,5,6,11,16]   # label (discrete numerical)\n",
        "ohe_features = categorical_features#[3,9,13,14,18,19]\n",
        "\n",
        "# features = ['Chck account','Duration','Credit History',\n",
        "#             'Purpose','Credit amount','Savings accnt/bonds','Employment',\n",
        "#             'Installment_rate','Sex','Debtors/guarantors','Residence',\n",
        "#             'Property','Age','Installment plans','Housing',\n",
        "#             'Number of Credits','Job',\n",
        "#             'Number of people being liable to provide maintenance for',\n",
        "#             'Telephone','Foreign Worker','Response']\n",
        "\n",
        "# read data from the cloud\n",
        "gds = pd.read_csv(path + 'German Dataset/germandata.csv', header = None, index_col = False)\n",
        "\n",
        "# gds.columns = features\n",
        "print (\"There are\", len(gds), \"data points.\\n\")\n",
        "\n",
        "#  public labels (Response)\n",
        "print('public label (Response)')\n",
        "gds_public = target_2label_processing(gds, 20)\n",
        "\n",
        "#  sensitive lables (index 8)\n",
        "gds_sensitive = []\n",
        "gds.loc[(gds[8] == 'A92', 8)] = 0 \n",
        "gds.loc[(gds[8] != 0, 8)] = 1\n",
        "print('sensitive label (Sex)')\n",
        "for value in gds[8]:\n",
        "        gds_sensitive.append(value)   \n",
        "gds_sensitive = np.asarray(gds_sensitive).reshape(-1,1)\n",
        "\n",
        "#  give categorical features labels\n",
        "categorial_feature_change(gds,categorical_features)\n",
        "\n",
        "#  numerical features and label feaures\n",
        "gds_std = []  \n",
        "gds_std = collect_columndata(gds,gds_std, numerical_features)\n",
        "gds_std = collect_columndata(gds,gds_std, label_features)\n",
        "#  zero mean, unit variance\n",
        "gds_std = zero_mean_unit_var(gds_std)\n",
        "print()\n",
        "print(len(gds_std[0]), 'numerical and label features:', gds_std.shape)\n",
        "\n",
        "#  one hot encoding some features\n",
        "gds_onehot, gds_onehot_labels = ohe_encoding(gds, ohe_features)\n",
        "print (len(gds_onehot_labels),'new one_hot labels from one hot enconding features:',gds_onehot.shape)\n",
        "\n",
        "#  clean data\n",
        "gds_cleandata = np.concatenate((gds_std, gds_onehot), axis = 1)\n",
        "gds_labels = np.concatenate((gds_public, gds_sensitive),axis = 1)\n",
        "\n",
        "print ('\\ncleandata shape:', gds_cleandata.shape)\n",
        "print ('public    labels (index 0):',gds_labels[:5,0], '%s percent is 1' %(np.mean(gds_labels[:,0])*100))\n",
        "print ('sensitive labels (index 1):', gds_labels[:5,1], '%s percent is 1' %(np.mean(gds_labels[:,1])*100))\n",
        "\n",
        "# for i in range(len(gds_cleandata[1])):\n",
        "#   print (i, np.mean(gds_cleandata[:,i]), '\\t', gds_cleandata[:5,i])"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1000 data points.\n",
            "\n",
            "public label (Response)\n",
            "0 = 1\n",
            "1 = 2\n",
            "sensitive label (Sex)\n",
            "\n",
            "13 numerical and label features: (1000, 13)\n",
            "50 new one_hot labels from one hot enconding features: (1000, 50)\n",
            "\n",
            "cleandata shape: (1000, 63)\n",
            "public    labels (index 0): [0 1 0 0 1] 30.0 percent is 1\n",
            "sensitive labels (index 1): [1 0 1 1 1] 69.0 percent is 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq55YflPAGZJ",
        "colab_type": "text"
      },
      "source": [
        "# **Adult Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7AlFtKeodGz",
        "colab_type": "text"
      },
      "source": [
        "Attributes:\n",
        "\n",
        "0. **age**: continuous.\n",
        "1. **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
        "2. **fnlwgt**: continuous.\n",
        "3. **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
        "4. **education-num**: continuous.\n",
        "5. **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
        "6. **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
        "7. **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
        "8. **race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
        "9. **sex**: Female, Male.\n",
        "10. **capital-gain**: continuous.\n",
        "11. **capital-loss**: continuous.\n",
        "12. **hours-per-week**: continuous.\n",
        "13. **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_t-SIRALI5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b06fcfd7-fa14-41c8-e7ab-3bcceb6438e8"
      },
      "source": [
        "# =============================== Variables ====================================\n",
        "\n",
        "numerical_features = [0,2,4,10,11,12]\n",
        "label_features = []\n",
        "categorial_features = [1,3,5,6,7,8,13]\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "ads = pd.read_csv(path + 'Adult Dataset/adult.data', header = None)\n",
        "print (\"There are\", len(ads), \"data points.\", \"Data sample %s: \\n\" %(200))\n",
        "# print (ads.loc[200],'\\n')\n",
        "print('age:', np.mean(ads[0]))\n",
        "\n",
        "#  sensitive data index = 9,  public label = 14\n",
        "print('public label', 14)\n",
        "ads_public = target_2label_processing(ads, 14)\n",
        "print('\\nsensitive label', 9)\n",
        "ads_sensitive = target_2label_processing(ads, 9)\n",
        "\n",
        "#  give categorical features labels\n",
        "categorial_feature_change(ads, categorial_features)\n",
        "\n",
        "#  numerical features and label features, \n",
        "ads_std = []\n",
        "ads_std = collect_columndata(ads, ads_std, numerical_features)\n",
        "ads_std = collect_columndata(ads, ads_std, label_features)\n",
        "#  zero mean, unit variance\n",
        "ads_std = zero_mean_unit_var(ads_std)\n",
        "\n",
        "#  one hot encoding categorical features\n",
        "ads_onehot, ads_onehot_labels = ohe_encoding(ads, categorial_features)\n",
        "\n",
        "#  clean data\n",
        "ads_cleandata = np.concatenate((ads_std, ads_onehot), axis = 1)\n",
        "ads_labels = np.concatenate((ads_public, ads_sensitive),axis = 1)\n",
        "\n",
        "print()\n",
        "print(len(ads_std[0]), 'numerical and label features:', ads_std.shape)\n",
        "print('\\ncleandata shape:', ads_cleandata.shape)\n",
        "print(len(ads_onehot_labels),'new one_hot labels from one hot enconding features:',ads_onehot.shape)\n",
        "print('public    labels (index 0):',ads_labels[:10,0], '%.2f percent is 1'%(np.mean(ads_labels[:,0])*100))\n",
        "print('sensitive labels (index 1):', ads_labels[:10,1], '%.2f percent is 1'%(np.mean(ads_labels[:,1])*100))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 32561 data points. Data sample 200: \n",
            "\n",
            "age: 38.58164675532078\n",
            "public label 14\n",
            "0 =  <=50K\n",
            "1 =  >50K\n",
            "\n",
            "sensitive label 9\n",
            "0 =  Male\n",
            "1 =  Female\n",
            "\n",
            "6 numerical and label features: (32561, 6)\n",
            "\n",
            "cleandata shape: (32561, 106)\n",
            "100 new one_hot labels from one hot enconding features: (32561, 100)\n",
            "public    labels (index 0): [0 0 0 0 0 0 0 1 1 1] 24.08 percent is 1\n",
            "sensitive labels (index 1): [0 0 0 0 1 1 1 0 1 0] 33.08 percent is 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyiyU8vd9xBd",
        "colab_type": "text"
      },
      "source": [
        "## German Training\n",
        "\n",
        "* For the much smaller German dataset we used 60 hidden units for both encoders and decoders. \n",
        "\n",
        "* A three-layer neural network with batch normalization [Ioffe and Szegedy, 2015] is employed for the discriminator.\n",
        "\n",
        "* We use a batch size of 16 and the number of hidden units is set to 64. γ is set to 1 in our experiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiTOv3IOi-r3",
        "colab_type": "text"
      },
      "source": [
        "#### raw data X"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUfCV2Kp7oIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "d0294798-b669-44ca-b219-725244ed78cf"
      },
      "source": [
        "# ============================================================================\n",
        "#      variables\n",
        "# ============================================================================\n",
        "\n",
        "input_size = gds_cleandata.shape[1]\n",
        "label_index = 1\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "hidden_size = [64,64,64]\n",
        "gamma = 1.0\n",
        "weight = 0 \n",
        "# [0.3,0.7]\n",
        "# for loss function bias\n",
        "\n",
        "\n",
        "print_every = 30000\n",
        "printing_result = False\n",
        "iterates = 1\n",
        "\n",
        "# ============================================================================\n",
        "#    variable ends\n",
        "# ============================================================================\n",
        "test_accuracies = []\n",
        "\n",
        "for iterate in range(iterates):\n",
        "  print(iterate, 'Building Model...')\n",
        "  # ============================================================================\n",
        "  #      variables\n",
        "  # ============================================================================\n",
        "\n",
        "  model = Decoder(input_size, 2, hidden_size[0], nn.Sigmoid()).double() if label_index == 0 else Discriminator(input_size, 2, hidden_size)\n",
        "  summary(model.float(), (1, input_size)) if iterate == 0 else None\n",
        "  train_loader, valid_loader, test_loader = loading_data(gds_cleandata, gds_labels, 0.13, 0.09, printing_result)\n",
        "\n",
        "  # ============================================================================\n",
        "  #      variables ends\n",
        "  # ============================================================================\n",
        "  original_training(model, train_loader, valid_loader, label_index, gamma, weight, learning_rate, epochs, batch_size, print_every, printing_result)\n",
        "  test_accuracies.append( original_testing(model, test_loader, label_index, printing_result, -1) )\n",
        "print ('Average Test Accuracy:', sum(test_accuracies)/len(test_accuracies))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Building Model...\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Sigmoid-1                [-1, 1, 64]               0\n",
            "             layer-2                [-1, 1, 64]           4,096\n",
            "           Sigmoid-3                [-1, 1, 64]               0\n",
            "             layer-4                [-1, 1, 64]           4,160\n",
            "           Sigmoid-5                [-1, 1, 64]               0\n",
            "             layer-6                [-1, 1, 64]           4,160\n",
            "             layer-7                 [-1, 1, 2]             130\n",
            "================================================================\n",
            "Total params: 12,546\n",
            "Trainable params: 12,546\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.05\n",
            "Estimated Total Size (MB): 0.05\n",
            "----------------------------------------------------------------\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 1\n",
            "loss: 2.283310615235267\n",
            "Test Accuracy: 0.6307692307692307 \n",
            "\n",
            "Average Test Accuracy: 0.6307692307692307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMEk2fLMi2Vi",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVZPqcBhi2kj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c155051-71cc-41a4-864d-2e97cdf2ebfe"
      },
      "source": [
        "# ============================================================================\n",
        "#      variables\n",
        "# ============================================================================\n",
        "\n",
        "input_size = gds_cleandata.shape[1]\n",
        "label_index = 0\n",
        "# representation_size = 100\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 25\n",
        "learning_rate = 0.001\n",
        "hidden_size = 64\n",
        "gamma = 1.0\n",
        "weight = 0 \n",
        "\n",
        "\n",
        "print_every = 10000\n",
        "printing_result = False\n",
        "iterates = 10\n",
        "\n",
        "hidden_size = [64,64,64]\n",
        "label_index = 0\n",
        "\n",
        "# ============================================================================\n",
        "#    variable ends\n",
        "# ============================================================================\n",
        "test_accuracies = []\n",
        "\n",
        "for iterate in range(iterates):\n",
        "  print(iterate, 'Building Model...')\n",
        "  # ============================================================================\n",
        "  #      variables\n",
        "  # ============================================================================\n",
        "  \n",
        "  encoder = Encoder(input_size, hidden_size[0], nn.Sigmoid())\n",
        "  decoder = Decoder(hidden_size[0],2, hidden_size[0], nn.Sigmoid()) if label_index == 0 else Discriminator(hidden_size[0], 2, hidden_size)\n",
        "  model = Combine_model(encoder, decoder).double()\n",
        "  \n",
        "  summary(model.float(), (1, input_size)) if iterate == 0 else None\n",
        "\n",
        "  train_loader, valid_loader, test_loader = loading_data(gds_cleandata, gds_labels, 0.09, 0.09, False)\n",
        "  # ============================================================================\n",
        "  #      variables ends\n",
        "  # ============================================================================\n",
        "\n",
        "  original_training(model, train_loader, valid_loader, label_index, gamma, weight, learning_rate, epochs, batch_size, print_every, printing_result)\n",
        "  test_accuracies.append( original_testing(model, test_loader, label_index, False, 20) )\n",
        "\n",
        "print ('Average Test Accuracy:', sum(test_accuracies)/len(test_accuracies))"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Building Model...\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Sigmoid-1                [-1, 1, 64]               0\n",
            "             layer-2                [-1, 1, 64]           4,096\n",
            "           Encoder-3                [-1, 1, 64]               0\n",
            "           Sigmoid-4                   [-1, 64]               0\n",
            "             layer-5                   [-1, 64]           4,160\n",
            "             layer-6                    [-1, 2]             130\n",
            "           Decoder-7                    [-1, 2]               0\n",
            "================================================================\n",
            "Total params: 8,386\n",
            "Trainable params: 8,386\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.03\n",
            "----------------------------------------------------------------\n",
            "Loading Dataset...\n",
            "   train dataset: (910, 63) (910, 2)\n",
            "   test  dataset: (90, 63) (90, 2)\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 1.9689645384042032\n",
            "Test Accuracy: 0.8111111111111111 \n",
            "\n",
            "1 Building Model...\n",
            "Loading Dataset...\n",
            "   train dataset: (910, 63) (910, 2)\n",
            "   test  dataset: (90, 63) (90, 2)\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 3.260325176588065\n",
            "Test Accuracy: 0.7888888888888889 \n",
            "\n",
            "2 Building Model...\n",
            "Loading Dataset...\n",
            "   train dataset: (910, 63) (910, 2)\n",
            "   test  dataset: (90, 63) (90, 2)\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 2.5062136830074757\n",
            "Test Accuracy: 0.7888888888888889 \n",
            "\n",
            "3 Building Model...\n",
            "Loading Dataset...\n",
            "   train dataset: (910, 63) (910, 2)\n",
            "   test  dataset: (90, 63) (90, 2)\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 0.9222824394632044\n",
            "Test Accuracy: 0.7444444444444445 \n",
            "\n",
            "4 Building Model...\n",
            "Loading Dataset...\n",
            "   train dataset: (910, 63) (910, 2)\n",
            "   test  dataset: (90, 63) (90, 2)\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 2.9903651318599995\n",
            "Test Accuracy: 0.7666666666666667 \n",
            "\n",
            "5 Building Model...\n",
            "Loading Dataset...\n",
            "   train dataset: (910, 63) (910, 2)\n",
            "   test  dataset: (90, 63) (90, 2)\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 3.079575355981088\n",
            "Test Accuracy: 0.8 \n",
            "\n",
            "6 Building Model...\n",
            "Loading Dataset...\n",
            "   train dataset: (910, 63) (910, 2)\n",
            "   test  dataset: (90, 63) (90, 2)\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 1.777389641278129\n",
            "Test Accuracy: 0.7777777777777778 \n",
            "\n",
            "7 Building Model...\n",
            "Loading Dataset...\n",
            "   train dataset: (910, 63) (910, 2)\n",
            "   test  dataset: (90, 63) (90, 2)\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 7.37386026974479\n",
            "Test Accuracy: 0.7222222222222222 \n",
            "\n",
            "8 Building Model...\n",
            "Loading Dataset...\n",
            "   train dataset: (910, 63) (910, 2)\n",
            "   test  dataset: (90, 63) (90, 2)\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 1.3667404150698266\n",
            "Test Accuracy: 0.7333333333333333 \n",
            "\n",
            "9 Building Model...\n",
            "Loading Dataset...\n",
            "   train dataset: (910, 63) (910, 2)\n",
            "   test  dataset: (90, 63) (90, 2)\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 5.078860080035591\n",
            "Test Accuracy: 0.7222222222222222 \n",
            "\n",
            "Average Test Accuracy: 0.7655555555555555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl_JbLop5WHg",
        "colab_type": "text"
      },
      "source": [
        "### NMT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le0tcDQgmZL3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a621b05b-981a-4dfc-b2ff-59dbe58bf24d"
      },
      "source": [
        "# ============================================================================\n",
        "#      variables\n",
        "# ============================================================================\n",
        "\n",
        "input_size = gds_cleandata.shape[1]\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 15\n",
        "learning_rate = 0.001\n",
        "hidden_size = 64\n",
        "gamma = 1.0\n",
        "weight = 0 \n",
        "loss_type = 'cel'\n",
        "hidden_size = [64,64,64]\n",
        "\n",
        "# verbose = 'dis'\n",
        "# verbose = 'dec'\n",
        "result_printing = False\n",
        "verbose = False\n",
        "print_every = 3000\n",
        "\n",
        "iterates = 8\n",
        "\n",
        "# ============================================================================\n",
        "#    variable ends\n",
        "# ============================================================================\n",
        "\n",
        "test_dec_acc = []\n",
        "test_dis_acc = []\n",
        "\n",
        "for iterate in range(iterates):\n",
        "    print(iterate, 'Building Model...')\n",
        "    # ============================================================================\n",
        "    #      variables\n",
        "    # ============================================================================\n",
        "\n",
        "    encoder = Encoder(input_size, hidden_size[0], nn.Sigmoid())\n",
        "    decoder = Decoder(hidden_size[0], 2, hidden_size[0], nn.Sigmoid())\n",
        "    discrminator = Discriminator(hidden_size[0], 2, hidden_size)\n",
        "\n",
        "    train_loader, valid_loader, test_loader = loading_data(gds_cleandata, gds_labels, 0.09, 0.09, result_printing)\n",
        "\n",
        "    # ============================================================================\n",
        "    #    variable ends\n",
        "    # ============================================================================\n",
        "\n",
        "    models, history_valid= main(train_loader, valid_loader, encoder, decoder, discrminator, loss_type, learning_rate, batch_size, epochs, print_every , result_printing)\n",
        "    dec_acc, dis_acc = Test_Model(models, test_loader, 10)\n",
        "    test_dec_acc.append(dec_acc)\n",
        "    test_dis_acc.append(dis_acc)\n",
        "\n",
        "print('Average Decoder Test Accuarcy:', sum(test_dec_acc)/len(test_dec_acc))\n",
        "print('Average Discrim Test Accuarcy:', sum(test_dis_acc)/len(test_dis_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Building Model...\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Sigmoid-1                [-1, 1, 64]               0\n",
            "             layer-2                [-1, 1, 64]           4,096\n",
            "           Encoder-3                [-1, 1, 64]               0\n",
            "           Sigmoid-4                [-1, 1, 64]               0\n",
            "             layer-5                [-1, 1, 64]           4,160\n",
            "           Sigmoid-6                [-1, 1, 64]               0\n",
            "             layer-7                [-1, 1, 64]           4,160\n",
            "           Sigmoid-8                [-1, 1, 64]               0\n",
            "             layer-9                [-1, 1, 64]           4,160\n",
            "            layer-10                 [-1, 1, 2]             130\n",
            "    Discriminator-11                 [-1, 1, 2]               0\n",
            "          Sigmoid-12                [-1, 1, 64]               0\n",
            "            layer-13                [-1, 1, 64]           4,160\n",
            "            layer-14                 [-1, 1, 2]             130\n",
            "          Decoder-15                 [-1, 1, 2]               0\n",
            "================================================================\n",
            "Total params: 20,996\n",
            "Trainable params: 20,996\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.09\n",
            "----------------------------------------------------------------\n",
            "Loading Optimizor...\n",
            "* number of parameters: 20996\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "Test Accuracies:: (pub)dec_acc: 0.711111, (sen)disc_acc: 0.722222\n",
            "\n",
            "1 Building Model...\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 20996\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "Test Accuracies:: (pub)dec_acc: 0.700000, (sen)disc_acc: 0.633333\n",
            "\n",
            "2 Building Model...\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 20996\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "Test Accuracies:: (pub)dec_acc: 0.733333, (sen)disc_acc: 0.611111\n",
            "\n",
            "3 Building Model...\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 20996\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "Test Accuracies:: (pub)dec_acc: 0.677778, (sen)disc_acc: 0.644444\n",
            "\n",
            "4 Building Model...\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 20996\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "Test Accuracies:: (pub)dec_acc: 0.722222, (sen)disc_acc: 0.555556\n",
            "\n",
            "5 Building Model...\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 20996\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "Test Accuracies:: (pub)dec_acc: 0.766667, (sen)disc_acc: 0.544444\n",
            "\n",
            "6 Building Model...\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 20996\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "Test Accuracies:: (pub)dec_acc: 0.700000, (sen)disc_acc: 0.588889\n",
            "\n",
            "7 Building Model...\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 20996\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "Test Accuracies:: (pub)dec_acc: 0.733333, (sen)disc_acc: 0.633333\n",
            "\n",
            "Average Decoder Test Accuarcy: 0.7180555555555556\n",
            "Average Discrim Test Accuarcy: 0.6166666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h7_9Uc_95Ta",
        "colab_type": "text"
      },
      "source": [
        "## Adult Training\n",
        "\n",
        "the sensitive lable is age! which loss function to use?\n",
        "\n",
        "For the Adult dataset both encoders, for z1 and z2, and both decoders, for z1 and x, had one hidden\n",
        "layer of 100 units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pszQMmGbnSE-",
        "colab_type": "text"
      },
      "source": [
        "#### raw data X"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJk9tFUXQaAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00cb972e-0a17-4469-c553-c69006f2bed2"
      },
      "source": [
        "# ============================================================================\n",
        "#      variables\n",
        "# ============================================================================\n",
        "\n",
        "input_size = ads_cleandata.shape[1]\n",
        "\n",
        "batch_size = 500\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "hidden_size = [100]\n",
        "gamma = 1.0\n",
        "weight = 0\n",
        "\n",
        " # for loss function bias\n",
        "\n",
        "\n",
        "print_every = 10000\n",
        "label_index = 0\n",
        "printing_result = False\n",
        "iterates = 8\n",
        "\n",
        "# ============================================================================\n",
        "#    variable ends\n",
        "# ============================================================================\n",
        "test_accuracies = []\n",
        "\n",
        "for iterate in range(iterates):\n",
        "  print(iterate, 'Building Model...')\n",
        "  # ============================================================================\n",
        "  #      variables\n",
        "  # ============================================================================\n",
        "\n",
        "  model = Decoder(input_size, 2, hidden_size[0], nn.Sigmoid())\n",
        "  # model = Discriminator(input_size, 2, hidden_size)\n",
        "\n",
        "  summary(model.float(), (1, input_size)) if iterate == 0 else None\n",
        "  train_loader, valid_loader, test_loader = loading_data(ads_cleandata, ads_labels, 0.09, 0.09, printing_result)\n",
        "\n",
        "  # ============================================================================\n",
        "  #      variables ends\n",
        "  # ============================================================================\n",
        "  original_training(model, train_loader, valid_loader, label_index, gamma, weight, learning_rate, epochs, batch_size, print_every, printing_result)\n",
        "  test_accuracies.append( original_testing(model, test_loader, label_index, printing_result) )\n",
        "\n",
        "print ('Average Test Accuracy:', sum(test_accuracies)/len(test_accuracies))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Building Model...\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Sigmoid-1               [-1, 1, 100]               0\n",
            "             layer-2               [-1, 1, 100]          10,700\n",
            "             layer-3                 [-1, 1, 2]             202\n",
            "================================================================\n",
            "Total params: 10,902\n",
            "Trainable params: 10,902\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.04\n",
            "----------------------------------------------------------------\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 1.4089055834055257\n",
            "Test Accuracy: 0.8502217673149096 \n",
            "\n",
            "1 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 2.306911865739632\n",
            "Test Accuracy: 0.8498805868304333 \n",
            "\n",
            "2 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 1.7577033860439835\n",
            "Test Accuracy: 0.8457864210167179 \n",
            "\n",
            "3 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 10.39493632736686\n",
            "Test Accuracy: 0.8324803821221426 \n",
            "\n",
            "4 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 9.851869097398538\n",
            "Test Accuracy: 0.8375980893892869 \n",
            "\n",
            "5 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 4.840120269904573\n",
            "Test Accuracy: 0.8474923234390993 \n",
            "\n",
            "6 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 3.230580735251182\n",
            "Test Accuracy: 0.8498805868304333 \n",
            "\n",
            "7 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 0\n",
            "loss: 1.316270235781043\n",
            "Test Accuracy: 0.8515864892528148 \n",
            "\n",
            "Average Test Accuracy: 0.8456158307744798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chkm1jWDnTt_",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKGg12YonUCm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d66136db-53c9-40d2-9178-3491b9c63052"
      },
      "source": [
        "# ============================================================================\n",
        "#      variables\n",
        "# ============================================================================\n",
        "\n",
        "input_size = ads_cleandata.shape[1]\n",
        "\n",
        "\n",
        "representation_size = 100\n",
        "\n",
        "batch_size = 500\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "hidden_size = [100]\n",
        "gamma = 1.0\n",
        "weight = 0\n",
        "\n",
        "label_index = 1\n",
        "print_every = 1000\n",
        "printing_result = False\n",
        "iterates = 8\n",
        "\n",
        "# ============================================================================\n",
        "#    variable ends\n",
        "# ============================================================================\n",
        "test_accuracies = []\n",
        "\n",
        "for iterate in range(iterates):\n",
        "  print(iterate, 'Building Model...')\n",
        "  # ============================================================================\n",
        "  #      variables\n",
        "  # ============================================================================\n",
        "\n",
        "  encoder = Encoder(input_size, representation_size, nn.Sigmoid())\n",
        "  decoder = Decoder(representation_size, 2, hidden_size[0], nn.Sigmoid())\n",
        "  model = Combine_model(encoder, decoder)\n",
        "\n",
        "  # encoder = Encoder(input_size, representation_size)\n",
        "  # discriminator = Discriminator(representation_size, 2, hidden_size)\n",
        "  # model = Combine_model(encoder, discriminator)\n",
        "  \n",
        "  summary(model.float(), (1, input_size)) if iterate == 0 else None\n",
        "\n",
        "  train_loader, valid_loader, test_loader = loading_data(ads_cleandata, ads_labels, 0.09, 0.09, False)\n",
        "  # ============================================================================\n",
        "  #      variables ends\n",
        "  # ============================================================================\n",
        "\n",
        "  original_training(model, train_loader, valid_loader, label_index, gamma, weight, learning_rate, epochs, batch_size, print_every, printing_result)\n",
        "  test_accuracies.append( original_testing(model, test_loader, label_index, False, 20) )\n",
        "\n",
        "print ('Average Test Accuracy:', sum(test_accuracies)/len(test_accuracies))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Building Model...\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Sigmoid-1               [-1, 1, 100]               0\n",
            "             layer-2               [-1, 1, 100]          10,700\n",
            "           Encoder-3               [-1, 1, 100]               0\n",
            "           Sigmoid-4               [-1, 1, 100]               0\n",
            "             layer-5               [-1, 1, 100]          10,100\n",
            "             layer-6                 [-1, 1, 2]             202\n",
            "           Decoder-7                 [-1, 1, 2]               0\n",
            "================================================================\n",
            "Total params: 21,002\n",
            "Trainable params: 21,002\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.08\n",
            "----------------------------------------------------------------\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 1\n",
            "loss: 6.891394390094967\n",
            "loss: 0.31649526630019537\n",
            "Test Accuracy: 0.8225861480723302 \n",
            "\n",
            "1 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 1\n",
            "loss: 1.3485050518829262\n",
            "loss: 0.2816396866728595\n",
            "Test Accuracy: 0.827021494370522 \n",
            "\n",
            "2 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 1\n",
            "loss: 1.1718637906099891\n",
            "loss: 0.266940136756529\n",
            "Test Accuracy: 0.8304332992152849 \n",
            "\n",
            "3 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 1\n",
            "loss: 1.7995469136735254\n",
            "loss: 0.26210885877370343\n",
            "Test Accuracy: 0.834186284544524 \n",
            "\n",
            "4 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 1\n",
            "loss: 1.785159924409135\n",
            "loss: 0.27207014687126807\n",
            "Test Accuracy: 0.8307744796997611 \n",
            "\n",
            "5 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 1\n",
            "loss: 5.5587405495251305\n",
            "loss: 0.29485170307768194\n",
            "Test Accuracy: 0.8259979529170931 \n",
            "\n",
            "6 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 1\n",
            "loss: 2.1107983958613796\n",
            "loss: 0.28161058353584184\n",
            "Test Accuracy: 0.8358921869669055 \n",
            "\n",
            "7 Building Model...\n",
            "Loading Dataset...\n",
            "\n",
            "Training...  Label: 1\n",
            "loss: 2.1799189104721095\n",
            "loss: 0.2681021353017242\n",
            "Test Accuracy: 0.8222449675878539 \n",
            "\n",
            "Average Test Accuracy: 0.8286421016717842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1p_K8geQaL2",
        "colab_type": "text"
      },
      "source": [
        "### NMT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA4wzYLk94TY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1cdf176a-9839-4f5e-de6d-19d43c019bb5"
      },
      "source": [
        "# ============================================================================\n",
        "#      variables\n",
        "# ============================================================================\n",
        "\n",
        "input_size = ads_cleandata.shape[1]\n",
        "\n",
        "representation_size = 100\n",
        "batch_size = 500\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "hidden_size = [100,100,100]\n",
        "gamma = -1.0\n",
        "weight = 0\n",
        "\n",
        "loss_type = 'cel'\n",
        "\n",
        "# verbose = 'dis'\n",
        "# verbose = 'dec'\n",
        "verbose = False\n",
        "print_every = 1000\n",
        "result_printing = False\n",
        "iterates = 6\n",
        "\n",
        "# ============================================================================\n",
        "#    variable ends\n",
        "# ============================================================================\n",
        "\n",
        "test_dec_acc = []\n",
        "test_dis_acc = []\n",
        "\n",
        "for iterate in range(iterates):\n",
        "\n",
        "    # ============================================================================\n",
        "    #      variables\n",
        "    # ============================================================================\n",
        "\n",
        "    encoder = Encoder(input_size, representation_size)\n",
        "    decoder = Decoder(representation_size, 2, hidden_size[0], nn.Sigmoid())\n",
        "    discrminator = Decoder(representation_size, 2, hidden_size[0])\n",
        "    # discrminator = Discriminator(representation_size, 2, hidden_size)\n",
        "\n",
        "    train_loader, valid_loader, test_loader = loading_data(ads_cleandata, ads_labels, 0.09, 0.09, result_printing)\n",
        "\n",
        "    # ============================================================================\n",
        "    #    variable ends\n",
        "    # ============================================================================\n",
        "\n",
        "    models, history_valid= main(train_loader, valid_loader, encoder, decoder, discrminator, loss_type, learning_rate, batch_size, epochs, print_every , verbose)\n",
        "    dec_acc, dis_acc = Test_Model(models, test_loader, 10, True)\n",
        "    test_dec_acc.append(dec_acc)\n",
        "    test_dis_acc.append(dis_acc)\n",
        "\n",
        "print('Average Decoder Test Accuarcy:', sum(test_dec_acc)/len(test_dec_acc))\n",
        "print('Average D  iscrim Test Accuarcy:', sum(test_dis_acc)/len(test_dis_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Dataset...\n",
            "Loading Model...\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Sigmoid-1               [-1, 1, 100]               0\n",
            "             layer-2               [-1, 1, 100]          10,700\n",
            "           Encoder-3               [-1, 1, 100]               0\n",
            "           Sigmoid-4               [-1, 1, 100]               0\n",
            "             layer-5               [-1, 1, 100]          10,100\n",
            "             layer-6                 [-1, 1, 2]             202\n",
            "           Decoder-7                 [-1, 1, 2]               0\n",
            "           Sigmoid-8               [-1, 1, 100]               0\n",
            "             layer-9               [-1, 1, 100]          10,100\n",
            "            layer-10                 [-1, 1, 2]             202\n",
            "          Decoder-11                 [-1, 1, 2]               0\n",
            "================================================================\n",
            "Total params: 31,304\n",
            "Trainable params: 31,304\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 0.12\n",
            "Estimated Total Size (MB): 0.13\n",
            "----------------------------------------------------------------\n",
            "Loading Optimizor...\n",
            "* number of parameters: 31304\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "loss: 0.349709, dec_acc: 0.848000, disc_acc: 0.336000\n",
            "\n",
            "public    tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n",
            "pub pred. tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n",
            "sensitive tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0])\n",
            "sen pred. tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Test Accuracies:: (pub)dec_acc: 0.851245, (sen)disc_acc: 0.322757\n",
            "\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 31304\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "loss: 0.351401, dec_acc: 0.846000, disc_acc: 0.664000\n",
            "\n",
            "public    tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n",
            "pub pred. tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n",
            "sensitive tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0])\n",
            "sen pred. tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Accuracies:: (pub)dec_acc: 0.842716, (sen)disc_acc: 0.677243\n",
            "\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 31304\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "loss: 0.334715, dec_acc: 0.858000, disc_acc: 0.336000\n",
            "\n",
            "public    tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n",
            "pub pred. tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1])\n",
            "sensitive tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0])\n",
            "sen pred. tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Test Accuracies:: (pub)dec_acc: 0.855339, (sen)disc_acc: 0.322757\n",
            "\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 31304\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "loss: 0.334904, dec_acc: 0.868000, disc_acc: 0.664000\n",
            "\n",
            "public    tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n",
            "pub pred. tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1])\n",
            "sensitive tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0])\n",
            "sen pred. tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Test Accuracies:: (pub)dec_acc: 0.849198, (sen)disc_acc: 0.677243\n",
            "\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 31304\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "loss: 0.333715, dec_acc: 0.850000, disc_acc: 0.336000\n",
            "\n",
            "public    tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n",
            "pub pred. tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1])\n",
            "sensitive tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0])\n",
            "sen pred. tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Test Accuracies:: (pub)dec_acc: 0.836233, (sen)disc_acc: 0.322757\n",
            "\n",
            "Loading Dataset...\n",
            "Loading Model...\n",
            "Loading Optimizor...\n",
            "* number of parameters: 31304\n",
            "\n",
            "Training Model....\n",
            "* Predictor     Criterion: cel\n",
            "* Discriminator Criterion: cel \n",
            "\n",
            "loss: 0.351014, dec_acc: 0.846000, disc_acc: 0.336000\n",
            "\n",
            "public    tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n",
            "pub pred. tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n",
            "sensitive tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0])\n",
            "sen pred. tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Test Accuracies:: (pub)dec_acc: 0.844763, (sen)disc_acc: 0.322757\n",
            "\n",
            "Average Decoder Test Accuarcy: 0.8465825088138293\n",
            "Average D  iscrim Test Accuarcy: 0.4409189127715229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL86bL1_4vdq",
        "colab_type": "text"
      },
      "source": [
        "# **Image Dataset**\n",
        "\n",
        "**Explanation:**\n",
        "* All filenames begin with the base name 'yaleB' followed by a two digit number signifying the **subject number (01 - 10)**.\n",
        "* The 2 digit number after '_P' signifies the **pose number (00 - 08)**. (See below for the relative pose positions.)\n",
        "\n",
        "For example, the image with the filename:\n",
        "\n",
        "           'yaleB03_P06A+035E+40.pgm'\n",
        "\n",
        "belongs to **subject #3** seen in **pose #6**, and the light source direction with respect to the camera axis is at **35 degrees azimuth** ('A+035') and **40 degrees elevation** ('E+40'). \n",
        "\n",
        "Note that a positive azimuth implies that the light source was to the right of the subject while negative means it was to the left. Positive elevation implies above the horizon, while negative implies below the horizon.\n",
        "\n",
        "\n",
        "**Lighting Condition Attributes:**\n",
        "\n",
        "* elevation: \n",
        "      +00, -10, +45, +40, -20, +90, +15, -35, +10, +65, +20, -40\n",
        "\n",
        "* azimuth:\n",
        "      +000E, +025E, +020E, +010E, +035E, +005E, +015E, +120E, -010E, -130E, -110E, +110E, +085E, -070E, -025E, +130E, -035E, -060E, -005E, -020E, +070E, +095E, -050E, +050E, -120E, -085E, -095E, +060E, -015E\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNHEr7uY8rU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# upper left(266) elevation: ['+40', '+15', '+65', '+20', '+45'] azimuth: ['+035E', '+015E', '+070E', '+085E', '+060E']\n",
        "# lower left (263): elevation: ['-10', '-20', '-40', '-35'] azimuth: ['+020E', '+035E', '+085E', '+070E', '+050E', '+060E']\n",
        "# upper right(265) elevation: ['+45', '+40', '+15', '+65', '+20'] azimuth: ['-070E', '-035E', '-015E', '-085E', '-060E']\n",
        "# lower right(263) elevation: ['-20', '-40', '-10', '-35'] azimuth: ['-060E', '-020E', '-085E', '-050E', '-070E', '-035E']\n",
        "# front(266) elevation: ['+00', '-10', '-20', '+10', '+20'] azimuth: ['+000E', '+005E', '-005E']\n",
        "\n",
        "\n",
        "lightings = ['upper_right', 'lower_right', 'lower_left', 'upper_left', 'front']\n",
        "people = ['01','02','03', '04','05','06','07','08','09','10','11','12','13','15','16','17','18','19','20',\n",
        "          '21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39' ]\n",
        "\n",
        "data_dic = {'lower_right':{'elevation':['-20', '-40', '-10', '-35'], \n",
        "                           'azimuth': ['-060E', '-020E', '-085E', '-050E', '-070E', '-035E']\n",
        "                           },\n",
        "            'upper_right':{'elevation':['+45', '+40', '+15', '+65', '+20'], \n",
        "                           'azimuth': ['-070E', '-035E', '-015E', '-085E', '-060E']\n",
        "                           },\n",
        "            'lower_left':{'elevation':['-10', '-20', '-40', '-35'], \n",
        "                          'azimuth':  ['+020E', '+035E', '+085E', '+070E', '+050E', '+060E']\n",
        "                          },\n",
        "            'upper_left':{'elevation':['+40', '+15', '+65', '+20', '+45'],\n",
        "                          'azimuth': ['+035E', '+015E', '+070E', '+085E', '+060E']\n",
        "                          },\n",
        "            'front':{'elevation':['+00', '-10', '-20', '+10', '+20'], \n",
        "                     'azimuth': ['+000E', '+005E', '-005E']\n",
        "                     },\n",
        "            }"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPTN_pb7HNER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "outputId": "2e4971f8-22cd-4afc-edb7-2359036ec4b0"
      },
      "source": [
        " def image_original_data_loadig(data_dic, lightings, people, image_size = 50, plotting = False, who = '02', each_row = 10):  \n",
        "\n",
        "    lighting_class, person_class, count = -1, -1, 0\n",
        "    elevation, azimuth = [], []\n",
        "\n",
        "    image_test_data, image_train_data = [], []\n",
        "    train_public_label, train_sensitive_label, image_train_labels = [], [], []\n",
        "    test_public_label, test_sensitive_label, image_test_labels = [], [], [] #public: person,  private: lighting/angles\n",
        "    print('Loading Image Database...')\n",
        "\n",
        "    for lighting in lightings:\n",
        "        lighting_class += 1\n",
        "        person_class, train_count, plot_count = -1, 0, 0\n",
        "        each_person = np.zeros(38)\n",
        "\n",
        "        if plotting:\n",
        "            print('%s:'%lighting)\n",
        "            fig=plt.figure(figsize=(20,250))\n",
        "            axes=[]\n",
        "            people = [who]\n",
        "\n",
        "        for person in people:\n",
        "            person_class += 1\n",
        "            folder = path + 'Image Dataset/CroppedYale/yaleB' + str(person) + '/'\n",
        "            length = len(os.listdir(folder))\n",
        "            \n",
        "            for file in os.listdir(folder):\n",
        "                \n",
        "                if file.endswith(\".pgm\") and (file[-12:-7] in data_dic[lighting]['azimuth']) and (file[-7:-4] in data_dic[lighting]['elevation']):\n",
        "                    \n",
        "                    image = plt.imread(folder + file)\n",
        "                    if image_size != 0:\n",
        "                      image = cv2.resize(image, dsize=(image_size, image_size), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "                    if not plotting:\n",
        "                      image = image.reshape(-1)\n",
        "                      if each_person[person_class] == 0:\n",
        "                        image_train_data.append(image) # train data: one picture of each lighting per person\n",
        "                        train_public_label.append(person_class)\n",
        "                        train_sensitive_label.append(lighting_class)\n",
        "                      else:\n",
        "                        image_test_data.append(image) # train data: one picture of each lighting per person\n",
        "                        test_public_label.append(person_class)\n",
        "                        test_sensitive_label.append(lighting_class)\n",
        "\n",
        "                      each_person[person_class] += 1\n",
        "                      train_count += 1\n",
        "                      count += 1\n",
        "                    \n",
        "                    if plotting: # get unique azimuth and elevation values\n",
        "                      axes.append(fig.add_subplot(length/each_row, each_row, plot_count+1))\n",
        "                      axes[-1].set_title(person + '_' + file[-12:-4])\n",
        "                      axes[-1].set_axis_off()\n",
        "                      plt.imshow(image, plt.cm.gray)       \n",
        "                      plot_count += 1\n",
        "                      elevation.append(file[-7:-4]) if file[-7:-4] not in elevation else None\n",
        "                      azimuth.append(file[-12:-7]) if file[-12:-7] not in azimuth else None\n",
        "\n",
        "                if plotting and plot_count == each_row:\n",
        "                  fig.tight_layout()    \n",
        "                  plt.show()\n",
        "                  plot_count = 0\n",
        "                  axes=[]\n",
        "                  fig=plt.figure(figsize=(20,250))\n",
        "        \n",
        "        if plotting:\n",
        "            fig.tight_layout()    \n",
        "            plt.show()    \n",
        "        else:\n",
        "            print('%s total images\\t %s %s train images \\tImage shape:'%(count, train_count, lighting), image.shape, '\\tmin # of pics per person %s'%(int(min(each_person))))\n",
        "\n",
        "    if not plotting:\n",
        "        image_test_data, image_train_data = np.array(image_test_data), np.array(image_train_data)\n",
        "        image_train_data = np.transpose(image_train_data)\n",
        "        image_train_data = zero_mean_unit_var(image_train_data)\n",
        "        # image_train_data = image_train_data - np.mean(image_train_data, axis = 0)\n",
        "        # print('training data shape: ', image_train_data.shape)\n",
        "        image_test_data = np.transpose(image_test_data)\n",
        "        image_test_data = zero_mean_unit_var(image_test_data)\n",
        "        # image_test_data = image_test_data - np.mean(image_test_data, axis = 0)\n",
        "\n",
        "        print('\\ntraining data shape: ', image_train_data.shape)\n",
        "        print('public    labels (index = 0):', train_public_label[:39])\n",
        "        print('sensitive labels (index = 1):', train_sensitive_label[:39])\n",
        "        print('\\ntesting   data shape: ', image_test_data.shape)\n",
        "        print('public    labels (index = 0):', test_public_label[:239])\n",
        "        print('sensitive labels (index = 1):', test_sensitive_label[:239])\n",
        "\n",
        "        # train data labels\n",
        "        train_sensitive_label = np.array(train_sensitive_label).reshape(-1,1)\n",
        "        train_public_label = np.array(train_public_label).reshape(-1,1)\n",
        "        image_train_labels = np.concatenate((train_public_label, train_sensitive_label),axis = 1)\n",
        "        # test data labels\n",
        "        test_sensitive_label = np.array(test_sensitive_label).reshape(-1,1)\n",
        "        test_public_label = np.array(test_public_label).reshape(-1,1)\n",
        "        image_test_labels = np.concatenate((test_public_label, test_sensitive_label),axis = 1)\n",
        "\n",
        "    return image_train_data, image_train_labels, image_test_data, image_test_labels\n",
        "\n",
        "\n",
        "#  loading data\n",
        "image_size = 0\n",
        "plotting = False\n",
        "image_train_data, image_train_labels, image_test_data, image_test_labels = image_original_data_loadig(data_dic, lightings, people, image_size, plotting, '38')\n",
        "\n",
        "if not plotting:\n",
        "  print('\\nexample')\n",
        "  if image_size != 0: \n",
        "    image = image_train_data[0].reshape(image_size,image_size)\n",
        "  else:\n",
        "    image = image_train_data[0].reshape(192, 168)\n",
        "  fig=plt.figure(figsize=(3,3))\n",
        "  plt.imshow(image, plt.cm.gray)\n",
        "  plt.show()\n",
        "  print('real data\\n', image)"
      ],
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Image Database...\n",
            "265 total images\t 265 upper_right train images \tImage shape: (32256,) \tmin # of pics per person 6\n",
            "528 total images\t 263 lower_right train images \tImage shape: (32256,) \tmin # of pics per person 6\n",
            "791 total images\t 263 lower_left train images \tImage shape: (32256,) \tmin # of pics per person 6\n",
            "1057 total images\t 266 upper_left train images \tImage shape: (32256,) \tmin # of pics per person 7\n",
            "1323 total images\t 266 front train images \tImage shape: (32256,) \tmin # of pics per person 7\n",
            "\n",
            "training data shape:  (190, 32256)\n",
            "public    labels (index = 0): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 0]\n",
            "sensitive labels (index = 1): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "\n",
            "testing   data shape:  (1133, 32256)\n",
            "public    labels (index = 0): [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
            "sensitive labels (index = 1): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "example\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "real data\n",
            " [[-0.91021566 -0.90863161 -0.90837238 ...  2.34643418  2.48146525\n",
            "   2.65522016]\n",
            " [-0.89898376 -0.90807187 -0.90300192 ...  2.3007156   3.33523332\n",
            "   3.42035519]\n",
            " [-0.90662025 -0.9158145  -0.92695741 ...  2.26987569  2.30271225\n",
            "   2.42662535]\n",
            " ...\n",
            " [-0.82869798 -0.84497645 -0.83707632 ... -0.45605288 -0.34349626\n",
            "  -0.29403508]\n",
            " [-0.77992784 -0.79582267 -0.80608888 ... -0.51472333 -0.34007941\n",
            "  -0.15176829]\n",
            " [-0.79093069 -0.80362545 -0.83021299 ... -0.43614882 -0.1175365\n",
            "   0.21892638]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGb6klD20k-E",
        "colab_type": "text"
      },
      "source": [
        "# **Image Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YzHDGsmoO8Z",
        "colab_type": "text"
      },
      "source": [
        "Potential Problem: too many classes\n",
        "\n",
        "**Controllable Invariance:** \n",
        "\n",
        "* We use a one-layer neural network for the encoder and a one-layer neural network for prediction. \n",
        "* The discriminator is a two-layer neural network with batch normalization. \n",
        "* The batch size is set to 16 and the hidden size is set to 100.\n",
        "\n",
        "\n",
        "**The Variational Fair Autoencoder** (https://arxiv.org/pdf/1511.00830.pdf) also employed in a similar fashion by Li et al. (2014). (Learning Unbiased Features): https://arxiv.org/pdf/1412.5244.pdf. We had one hidden layer with 500, 400 units respectively for the z1 encoder, x decoder, and 300, 100 units respectively for the z2 encoder and z1 decoder.\n",
        "\n",
        "**Learning Unbiased Features:** In the experiment we used the extended Yale B dataset, which contains faces of 38 people under various lighting conditions corresponding to light source from different directions. We created 5 groups of images, corresponding to light source in upper right, lower right, lower left, upper left and the front. For each group of images, we chose 1 image per person to form one domain for that lighting condition. In this way we had 5 domains with 5 × 38 = 190 images in total. All the other images (around 2000) are used for testing. The task is to recognize the identity of the person in image, i.e. a 38-way classification task. For this task, we did not use a validation set, but rather report the best result on test set to see where the limits of different models are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNG4ei5J0jcn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30d44a84-9bb3-4d8e-f838-07dc83c296a4"
      },
      "source": [
        "# ============================================================================\n",
        "#      variables\n",
        "# ============================================================================\n",
        "\n",
        "input_size = image_train_data.shape[1]\n",
        "print('input size:', input_size)\n",
        "label_index = 0\n",
        "num_of_class = len(np.unique(image_train_labels[:,label_index]))\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 55\n",
        "learning_rate = 0.001\n",
        "gamma = 1.0\n",
        "weight = [1.0]*num_of_class \n",
        "hidden_size = [100,100,100]\n",
        "\n",
        "print_every = 5\n",
        "printing_result = True\n",
        "iterates = 1\n",
        "\n",
        "# ============================================================================\n",
        "#    variable ends\n",
        "# ============================================================================\n",
        "test_accuracies = []\n",
        "\n",
        "for iterate in range(iterates):\n",
        "  print(iterate, 'Building Model...')\n",
        "  # ============================================================================\n",
        "  #      variables\n",
        "  # ============================================================================\n",
        "\n",
        "  # encoder = Encoder(input_size, hidden_size[0], nn.ReLU())\n",
        "  # decoder = Decoder(hidden_size[0], num_of_class, hidden_size[1], nn.ReLU())\n",
        "  # decoder = Discriminator(hidden_size[0], num_of_class, hidden_size) \n",
        "  # model = Combine_model(encoder, decoder)\n",
        "  model = Decoder(input_size, num_of_class, hidden_size[1])\n",
        "\n",
        "  # encoder = Encoder_CNN(1, hidden_size[0])\n",
        "  # decoder = Decoder_CNN(hidden_size[0], num_of_class, hidden_size[1], 3, 1849920)\n",
        "  # model = Combine_model(encoder, decoder)\n",
        "  \n",
        "  summary(model.float(), (1, input_size)) if iterate == 0 else None\n",
        "  train_loader, valid_loader, test_loader = loading_data(image_train_data, image_train_labels, 0.1, 0.01, False, printing_result, False, False)\n",
        "\n",
        "  test_x, test_y = torch.from_numpy(image_test_data).double(), torch.from_numpy(image_test_labels)\n",
        "  test_ds = torchdata.TensorDataset(test_x, test_y)\n",
        "  test_loader = torchdata.DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # break\n",
        "  # # ============================================================================\n",
        "  #      variables ends\n",
        "  # ============================================================================\n",
        "  original_training(model, train_loader, valid_loader, label_index, gamma, weight, learning_rate, epochs, batch_size, print_every, printing_result)\n",
        "  test_accuracies.append( original_testing(model, test_loader, label_index, printing_result, -1) )\n",
        "\n",
        "print ('Average Test Accuracy:', sum(test_accuracies)/len(test_accuracies)) if len(test_accuracies) != 0 else None"
      ],
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size: 32256\n",
            "0 Building Model...\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "             layer-1                  [-1, 100]       3,225,700\n",
            "             layer-2                   [-1, 38]           3,838\n",
            "================================================================\n",
            "Total params: 3,229,538\n",
            "Trainable params: 3,229,538\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.12\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 12.32\n",
            "Estimated Total Size (MB): 12.44\n",
            "----------------------------------------------------------------\n",
            "Loading Dataset...\n",
            "   train dataset: (190, 32256) (190, 2)\n",
            "\n",
            "Training...  Label: 0\n",
            "Epoch: 1/55\n",
            "loss: 3502.0192234526735\n",
            "loss: 3243.4419954291952\n",
            "loss: 1699.9579177993844\n",
            "Training Accuract: 0.031578947368421054\n",
            "Epoch: 2/55\n",
            "loss: 1697.3975313356464\n",
            "loss: 1733.4722233883479\n",
            "Training Accuract: 0.06842105263157895\n",
            "Epoch: 3/55\n",
            "loss: 1624.89858714713\n",
            "loss: 1823.6173150051047\n",
            "loss: 1085.1619498757516\n",
            "Training Accuract: 0.1\n",
            "Epoch: 4/55\n",
            "loss: 1370.9218444405203\n",
            "loss: 1040.2332818919883\n",
            "Training Accuract: 0.18947368421052632\n",
            "Epoch: 5/55\n",
            "loss: 1234.0749808086146\n",
            "loss: 642.4769273065923\n",
            "Training Accuract: 0.2578947368421053\n",
            "Epoch: 6/55\n",
            "loss: 826.7234693900864\n",
            "loss: 727.1220447474324\n",
            "loss: 125.40173092543512\n",
            "Training Accuract: 0.33157894736842103\n",
            "Epoch: 7/55\n",
            "loss: 471.62596146382555\n",
            "loss: 297.2039961749242\n",
            "Training Accuract: 0.45263157894736844\n",
            "Epoch: 8/55\n",
            "loss: 232.37062597583554\n",
            "loss: 718.2836056297803\n",
            "loss: 161.92917124156367\n",
            "Training Accuract: 0.46842105263157896\n",
            "Epoch: 9/55\n",
            "loss: 332.18131898375367\n",
            "loss: 343.0865080944225\n",
            "Training Accuract: 0.5578947368421052\n",
            "Epoch: 10/55\n",
            "loss: 263.31338442808834\n",
            "loss: 120.88887400986546\n",
            "Training Accuract: 0.6263157894736842\n",
            "Epoch: 11/55\n",
            "loss: 181.49439451556606\n",
            "loss: 328.9172108862699\n",
            "loss: 11.183976634707719\n",
            "Training Accuract: 0.6789473684210526\n",
            "Epoch: 12/55\n",
            "loss: 238.6800103316266\n",
            "loss: 82.5439292732804\n",
            "Training Accuract: 0.6842105263157895\n",
            "Epoch: 13/55\n",
            "loss: 30.81141944750842\n",
            "loss: 357.7463812671057\n",
            "loss: 18.90840380383533\n",
            "Training Accuract: 0.7473684210526316\n",
            "Epoch: 14/55\n",
            "loss: 126.16294883125087\n",
            "loss: 10.875516377701697\n",
            "Training Accuract: 0.7578947368421053\n",
            "Epoch: 15/55\n",
            "loss: 212.26480689831982\n",
            "loss: 7.313672350076416\n",
            "Training Accuract: 0.7894736842105263\n",
            "Epoch: 16/55\n",
            "loss: 95.2271210499966\n",
            "loss: 20.087909567750927\n",
            "loss: 0.0\n",
            "Training Accuract: 0.8578947368421053\n",
            "Epoch: 17/55\n",
            "loss: 43.97116527361426\n",
            "loss: 1.7945029908486914\n",
            "Training Accuract: 0.868421052631579\n",
            "Epoch: 18/55\n",
            "loss: 0.008868753310899046\n",
            "loss: 109.09923936308299\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9052631578947369\n",
            "Epoch: 19/55\n",
            "loss: 1.1144438753782542\n",
            "loss: 30.673903029466715\n",
            "Training Accuract: 0.9315789473684211\n",
            "Epoch: 20/55\n",
            "loss: 6.123284608470243\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9578947368421052\n",
            "Epoch: 21/55\n",
            "loss: 10.380716894758791\n",
            "loss: 4.718447854656898e-16\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9526315789473684\n",
            "Epoch: 22/55\n",
            "loss: 1.0208500711419978e-13\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9789473684210527\n",
            "Epoch: 23/55\n",
            "loss: 0.0\n",
            "loss: 4.163336342344336e-17\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9736842105263158\n",
            "Epoch: 24/55\n",
            "loss: 14.052018247945467\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9578947368421052\n",
            "Epoch: 25/55\n",
            "loss: 0.0\n",
            "loss: 11.359981590980425\n",
            "Training Accuract: 0.9368421052631579\n",
            "Epoch: 26/55\n",
            "loss: 0.0\n",
            "loss: 4.921266555003257\n",
            "loss: 6.73962701817476\n",
            "Training Accuract: 0.9210526315789473\n",
            "Epoch: 27/55\n",
            "loss: 0.0\n",
            "loss: 9.984987621072719\n",
            "Training Accuract: 0.9631578947368421\n",
            "Epoch: 28/55\n",
            "loss: 2.5507291696090135\n",
            "loss: 0.0\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9736842105263158\n",
            "Epoch: 29/55\n",
            "loss: 0.29387229487717265\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9789473684210527\n",
            "Epoch: 30/55\n",
            "loss: 11.821009161527712\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9789473684210527\n",
            "Epoch: 31/55\n",
            "loss: 1.6388509972039042e-10\n",
            "loss: 0.0\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9789473684210527\n",
            "Epoch: 32/55\n",
            "loss: 9.202931627065894\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9894736842105263\n",
            "Epoch: 33/55\n",
            "loss: 1.1493832585480776e-09\n",
            "loss: 2.59491060661387e-07\n",
            "loss: 0.0\n",
            "Training Accuract: 1.0\n",
            "Epoch: 34/55\n",
            "loss: 5.0237591864268145e-14\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9894736842105263\n",
            "Epoch: 35/55\n",
            "loss: 4.024644987374828\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9842105263157894\n",
            "Epoch: 36/55\n",
            "loss: 0.0\n",
            "loss: 0.0\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9894736842105263\n",
            "Epoch: 37/55\n",
            "loss: 0.0\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9789473684210527\n",
            "Epoch: 38/55\n",
            "loss: 0.0\n",
            "loss: 6.778287259332558\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9842105263157894\n",
            "Epoch: 39/55\n",
            "loss: 0.0\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9789473684210527\n",
            "Epoch: 40/55\n",
            "loss: 0.0\n",
            "loss: 2.373101715136227e-15\n",
            "Training Accuract: 0.9631578947368421\n",
            "Epoch: 41/55\n",
            "loss: 0.0\n",
            "loss: 18.086740788655717\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9526315789473684\n",
            "Epoch: 42/55\n",
            "loss: 32.62590018956686\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9473684210526315\n",
            "Epoch: 43/55\n",
            "loss: 0.0\n",
            "loss: 32.3912845175776\n",
            "loss: 0.0\n",
            "Training Accuract: 0.968421052631579\n",
            "Epoch: 44/55\n",
            "loss: 6.199089622616015\n",
            "loss: 0.8202100547270005\n",
            "Training Accuract: 0.9578947368421052\n",
            "Epoch: 45/55\n",
            "loss: 0.0\n",
            "loss: 15.374750611520284\n",
            "Training Accuract: 0.9578947368421052\n",
            "Epoch: 46/55\n",
            "loss: 33.001904570060546\n",
            "loss: 0.0\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9578947368421052\n",
            "Epoch: 47/55\n",
            "loss: 0.0\n",
            "loss: 13.267416912750122\n",
            "Training Accuract: 0.9473684210526315\n",
            "Epoch: 48/55\n",
            "loss: 0.0\n",
            "loss: 0.0\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9578947368421052\n",
            "Epoch: 49/55\n",
            "loss: 0.0\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9842105263157894\n",
            "Epoch: 50/55\n",
            "loss: 9.034439862886558e-15\n",
            "loss: 3.221444783485083\n",
            "Training Accuract: 0.9631578947368421\n",
            "Epoch: 51/55\n",
            "loss: 0.0\n",
            "loss: 0.613563038854857\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9842105263157894\n",
            "Epoch: 52/55\n",
            "loss: 0.0\n",
            "loss: 1.3068338724475594\n",
            "Training Accuract: 0.9578947368421052\n",
            "Epoch: 53/55\n",
            "loss: 40.869813416957044\n",
            "loss: 6.294708371273437\n",
            "loss: 0.0\n",
            "Training Accuract: 0.9736842105263158\n",
            "Epoch: 54/55\n",
            "loss: 0.0\n",
            "loss: 20.581093209431742\n",
            "Training Accuract: 0.9631578947368421\n",
            "Epoch: 55/55\n",
            "loss: 0.18147445149596514\n",
            "loss: 9.494228525902116\n",
            "Training Accuract: 0.9631578947368421\n",
            "\n",
            "Real label: tensor([ 5, 34, 20,  4, 15, 28, 19,  4, 24, 33, 14, 29, 23, 34, 32])\n",
            "Pred label: tensor([ 5, 32, 20,  4, 15,  5, 19,  4, 23, 33,  5, 29, 23, 34, 32])\n",
            "Test Accuracy: 0.531332744924978 (total:1133)\n",
            "\n",
            "Average Test Accuracy: 0.531332744924978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBe8N92y0kTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMAgsL0D0kWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zde2Y1p5-KuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder_CNN(nn.Module):\n",
        "    def __init__(self, input_channel = 10, num_filters = 1, kernel_size = 3):\n",
        "        super(Encoder_CNN, self).__init__()\n",
        "        \n",
        "        # layers (CNN)\n",
        "        self.C1 = nn.Conv2d(input_channel, num_filters, kernel_size=kernel_size)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        o = self.C1(X)\n",
        "        return o\n",
        "\n",
        "    def flatten(self, x):\n",
        "        # print (x.size())\n",
        "        B, _, _, _ = x.size()\n",
        "        return x.contiguous().view(B,-1)\n",
        "\n",
        "class Decoder_CNN(nn.Module):\n",
        "    def __init__(self, input_dim = 1, output_dim = 3, num_filters = 1, kernel_size = 3, linear_input = 924960):\n",
        "        super(Decoder_CNN, self).__init__()\n",
        "        \n",
        "        # layers (CNN)\n",
        "        self.C1 = nn.Conv2d(input_dim, num_filters, kernel_size=kernel_size)\n",
        "        self.L1 = nn.Linear(linear_input, output_dim)\n",
        "        self.a1 = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, X):\n",
        "        self.z = self.C1(X)\n",
        "        self.z = self.a1(self.z)\n",
        "        self.z = self.flatten(self.z)\n",
        "        # print(self.z.size())\n",
        "        self.z = self.L1(self.z)\n",
        "        o = self.z\n",
        "        return o\n",
        "\n",
        "    def flatten(self, x):\n",
        "        # print (x.size())\n",
        "        B, _, _, _ = x.size()\n",
        "        return x.contiguous().view(B,-1)\n",
        "\n",
        "class Discriminator_CNN(nn.Module):\n",
        "    def __init__(self, input_dim = 1, output_dim = 3, hidden_filters = 1, num_filters = 1, kernel_size = 3, linear_input = 451980):\n",
        "        super(Discriminator_CNN, self).__init__()\n",
        "    \n",
        "        # layers (CNN)\n",
        "        self.C1 = nn.Conv2d(input_dim, hidden_filters, kernel_size=kernel_size)\n",
        "        self.BN = nn.BatchNorm2d(hidden_filters)\n",
        "        self.C2 = nn.Conv2d(hidden_filters, num_filters, kernel_size=kernel_size)\n",
        "        self.L1 = nn.Linear(linear_input, output_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z = self.C1(X)\n",
        "        self.z = self.BN(self.z)\n",
        "        self.z = self.C2(self.z)\n",
        "        self.z = F.relu(self.z)\n",
        "        self.z = self.flatten(self.z)\n",
        "        self.z = self.L1(self.z)\n",
        "        o = self.z\n",
        "        return o\n",
        "\n",
        "    def flatten(self, x):\n",
        "        # print (x.size())\n",
        "        B, N, _, _ = x.size()\n",
        "        return x.contiguous().view(B,-1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}